# Alberto Geniola, albertogeniola@gmail.com
# TheThing configuration file, June 2017, Helsinki
#
# 
# This is the main configuration file used to setup the HostController service. 
# Before touching anything here, please be sure to read the documentation of this system.
# We strongly suggest to stick with the defaults and to configure the network as shown in
# the tutorials. This configuration file has been set up to reflect the defaults used in the
# docs for each different operation mode (1 tier, 2 tiers and N-tiers).  

[general]
# The following represents the unique identifier associated to this host controller in the entire infrastructure.
# When using multiple host controllers, be sure each of them is using a different identifier in its configuration
# file.
host_controller_id = 1

# Connection string to the central database.
db_connection_string = postgresql+psycopg2://host_controller_agent:installeranalyzer@localhost/analyzer

# Which address to bind on. Specifying 0.0.0.0 will bind on all available interfaces.
# Note that some topologies might require to listen on the public internet (e.g. when using remote openstack clouds).
# In such cases, be sure your NAT/Firewall is configured accordingly.
bind_host_address = 0.0.0.0
bind_host_port = 9000

# Root folder where to download/fetch installers
installers_base_dir = C:\InstallAnalyzer\Installers

# Path where to store obtained reports
output_report_dir = C:\InstallAnalyzer\OutputReports

# Which directory should contain logging files. 
# The running process should have write access to such directory.
logs_directory=C:\InstallAnalyzer\logs

# Verbosity of logging: DEBUG/INFO/WARNING/ERROR
log_level = INFO

# Network analyzer. 
# A network analyzer is an optional service usually installed together with the sniffer. While sniffer simply collects data,
# the network analyzer performs deep inspection of the network capture, extracting relevant information and producing some
# .json report. In most cases, the network result analyzer is installed alongside the sniffer service.
# Use the following switches to configure the network analyzer service. If you have installed the network analyzer alongside the
# sniffer (which is the default), you probably want to enable the analyzer and set its IP to the IP of the sniffer node in your
# topology.
#
# Example 1: VirtualBox single tier topology (as for documented example)
# enable_analyzer = True
# network_analyzer_ip = 192.168.56.1
# network_analyzer_port = 7778
#
# Example 2: Openstack two tier topology (as for documented example)
# enable_analyzer = True
# network_analyzer_ip = <public ip of host controller>
# network_analyzer_port = 9090
#
# Example 3: BareMetal N-tier topology (as for documented example)
# enable_analyzer = True
# network_analyzer_ip = 192.168.0.1
# network_analyzer_port = 9090
#
# By default, we assume the user is using the Virtualbox 1-tier topology.
enable_analyzer = True
network_analyzer_ip = 192.168.56.1
network_analyzer_port = 7778

# Timeout for single analysis, in milliseconds. If a worker hits this value, the watchdog will force its sandbox power-cycle.
vm_run_timeout = 1500000

# In order to facilitate maintainability, sandboxes automatically retrieve their Sandbox Agent client at bootstrap.
# The HostController is in charge of publishing such clients to the sniffer: the sniffer in turn will serve those
# to sandbox clients when they start. Each Sandbox Agent is deeply platform dependant. Therefore the HostController
# should provide one or more agents, specific to the OS of the Sandboxes.
# The following parameter tells to the HostController where to look for Sandbox Agents. It must point to a directory
# containing as many sub directory as many OS types used during the analysis. Each level of the FS corresponds to a
# characteristic of the OS. The first level represents the name of the platform (Windows, Linux, etc). The second one
# identifies the OS version (7, 8, 10 for Windows) and the third one specifies the architecture. THose values can be
# easily retrieved via a python controle, using the platform module: platform.system(), platform.version(),
# platform.machine().
# An example of direcotry structure is the following.
# C:
# |--InstallAnalyzer
# |        |--...
# |        |--Agents
# |             |--Windows
# |                 |--7
# |                     |--x86
# |                         |--agent.zip
#
agents_dir=C:\InstallAnalyzer\Agents

# The following field must contain a list (also a single value is ok) of machine_managers to use.
# Possible values are: vbox, openstack, baremetal. Default is to use the vbox manager.
# managers=openstack
# managers=baremetal
managers=vbox



#############################################################################
################################ VirtualBox #################################
#############################################################################
[vbox]
# Currently, virtualbox manager supports two different operation modes: XP/COM and WebService. THe former, enable faster
# management of virtual machines and is the recommended mode for now. The second one, allows to use a remote webservice
# for handling virtualbox instances via the vboxwebservice. By default, the XP/COM mode is used. If you wish to use the
# vboxwebsrv mode, you need to specify a valid vbox_url, together with a valid user and password for authentication.
# Moreover you also need to manually start the vboxwebsrv and configure it using one supported authentication method.
;vbox_url = http://localhost:18083
;vbox_user = vboxuser
;vbox_password = 
# When vbox_url is commented or not available, the manager will use local XP/COM to talk with the Virtualbox service.
# This is the default and the recommended mode for beginners.

# Number of VM to spawn/handle during the analysis process
vbox_workers = 2

# The base disk used as clean state image for guests. 
# This image will be set as immutable, so that the rollback operation is simply obtained via shutdown. Every time a machine
# boots up, a differential disk is created (if not available) on the top of the base image. When the machine is powered off 
# and then started again, the differential disk gets wiped, so that the machine starts again from a clean state.
# For maximum performance, you might want to put the basedisk on a fast storage device or even a ramdisk.
vbox_base_disk_path = C:\InstallAnalyzer\Disks\guest_preparation.vdi

# Directory where to store differential disks for each VM. This can also be a ramdisk path.
vbox_diff_disk_dir = C:\InstallAnalyzer\Disks\Differential

# The following parameters describe which physical adapter should be used on the host as HostOnlyAdapter and
# how should it be configurated. On Windows, it generally is something like "VirtualBox Host-Only Ethernet Adapter #X"
# Be sure to check whether this is the case or not. You can add a new HostOnly adapter by using the command:
# $ VBoxManage hostonlyif create
vbox_host_only_interface_name = VirtualBox Host-Only Ethernet Adapter #2
vbox_host_only_interface_ip=192.168.56.1
vbox_host_only_interface_mask=255.255.255.0

# NAT configuration.
# In the single tier topology, the HostController uses a NAT network in order to provide internet access to the Sniffer and hence
# to the sandboxes. 
vbox_wan_nat_name=AnalyzerWan
vbox_sandboxes_internal_nat_name=AnalyzerInternalNet
vbox_wan_nat_cidr=172.16.0.0/24

# The following represents the name of the internal network used by the sandboxes.
vbox_intranet_network_name = TestNet

# According to the single tier topology, the host controller agent can reach the sniffer via the host-only adapter.
# The sniffer will bind 192.168.56.2 on ETH2 while the host binds 192.168.56.1.
vbox_sniffer_ip=192.168.56.2
vbox_sniffer_port=8080

# Default VM configuration for each guest.
vbox_default_vm_cfg = {
            "cpu_count": 2,
            "memory_size": 4096,
            "vram_size": 32,
            "accelerate_3d_enabled": 1,
            "accelerate_2d_video_enabled": 1,
            "adapter_intranet_type":"Virtio",
            "adapter_intranet_attachment":"Internal"
        }

# The group in which those VMs will be spawned handled. The group name should start with "/".
# !WARNING! If you choose an existing group name, the machine manager might destroy/remove all machines
# available in that group. Usage of a dedicated group name is advised.
vbox_default_group = /test

# Specifies the name VM to be used as sniffer. This name should be unique to avoid any kind of ambiguity.
# Note that this VM should not reside in the same VM group of test vms.
vbox_sniffer_name = sniffer
sniffer_base_disk = C:\InstallAnalyzer\Disks\sniffer.vdi
vbox_sniffer_cfg = {
            "cpu_count": 2,
            "memory_size": 8192,
            "vram_size": 32,
            "accelerate_3d_enabled": 0,
            "accelerate_2d_video_enabled": 0,
            "adapter_internet_type":"Virtio",
            "adapter_intranet_type":"Virtio",
            "adapter_hostonly_type":"Virtio"
        }



#############################################################################
################################# OPENSTACK #################################
#############################################################################
[openstack]
# When using the openstack cloud for performing analysis, the user has to specify credentials for the remote cloud.
os_auth_url=
os_project_name=
os_username=
os_password=

# How many workers do we want to run?
os_workers = 3

# Sniffer instance configuration:
# Which name whould be assigned to the sniffer instance?
os_sniffer_instance_name = InstallerAnalyzerSniffer

# What kind of flavor/resource set should it acquire?
os_sniffer_flavor = standard.medium

# Specify the name/GUUID of the image to be used with the sniffer. The user must have uploaded a prepared version of the sniffer,
# as explained in the tutorial. For more info, refer to the documentation.
os_sniffer_image_name = SnifferImage

# Similarly, we also need to address the image name to use as boot image for the sandbox
os_guest_image_name = SandboxImage
os_guest_flavor = standard.small

# Specify the name of the network used as external access to the internet
os_public_network_name = main_network

# Specify the name of the network used as intranet among sniffers and sandboxes
os_internal_network_name = InstallerAnalyzerNat
os_intranet_subnetwork_name = GuestIntranet

# Network configuration of intranet. Be advised: this should match the configuration of the sniffer image.
os_internal_network_cidr = 192.168.0.0/24

# Specify here the name (or the GUID) of the router attached to external network. This router is the one connected
# to the external internet and usually has a public IP address on its interface.
os_external_router_name =

# Port where to contact the sniffer. Note that the address of the sniffer will be automatically retrieved
# by the manager.
os_sniffer_port = 8080

# The sniffer security group name. This security group will be granted outgoing connectivity and incoming TCP connectivity
# in accordance with os_sniffer_port.
os_sniffer_sg = sniffer
os_guest_security_group = sandboxes

# Specify the external IP of the HOST controller. Such IP:PORT address will be used by the sniffer and clients
# to contact the host controller. The value to be assigned to these parameters depends on the specific network topology
# adopted.
os_external_hc_ip =
os_external_hc_port = 9000



#############################################################################
################################ BareMetal ##################################
#############################################################################
[baremetal]
# The baremetal manager currently works only on Windows hosts and makes use of iSCSI services
# offered by the operating system. It also supports only TP-LINK HS100 smart plugs as power
# managment system for bare-metal sandboxes.

# The following json array is used to configure Sandboxes in the manager.
# Each object in the array must contain the sandbox_mac and the samrtplug_ip properties and represents the mapping
# among smartplugs and associated hardware nodes. In other words, the manager will lookup this dictionary when it 
# needs to reboot/start a particular worker, which is identified by a specific MAC address.
baremetal_machines_conf = [{"sandbox_mac":"XX-XX-XX-XX-XX-XX", "smartplug_ip":"192.168.0.X"}]

# Path where to store differential disks for each worker. This directory must exist.
# For best performance, you might use a ramdisk or a fast storage device.
baremetal_diff_vhd_folder = C:\InstallAnalyzer\Disks

# Path to the base-disk to be used as clean state for images.
baremetal_base_vhd_path = C:\InstallAnalyzer\Disks\base_disk.vhd

# The following IP:PORT represents the address used by Sandboxes to reach the HostController.
# Obviously, they strongly depends on the specific network topology in place.
baremetal_external_hc_ip = 192.168.0.1
baremetal_external_hc_port = 9000

# The following value represents the URL of the sniffer service to be used in the baremetal configuration.
# It must be a valid URL, beginning with http:// or https://. Note, by default, the sniffer service listens on port
# 8080. So the url should be something like this: http://<IP>:8080
baremetal_sniffer_url = http://192.168.0.1:8080

# Address of the iSCSI server. In most of the cases, this is just the IP address of the HostController in the
# network where both HostController and Sandboxes reside.
baremetal_iscsi_server_ip = 192.168.1.251

# Address of the webservice that handles iscsi target creation/restoration, responding to requests performed by iPXE
# clients. Again, by default, this is the IP address of the HostController.
baremetal_websrv_host = 192.168.1.251
baremetal_websrv_port = 8181
